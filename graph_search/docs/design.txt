好友/关注人/单个用户赞过的微博实现方案
背景
每天微博赞数据(attitude)约1200万，历史数据120G（近1年数据，15亿），为了减轻graph search的pipe压力，根据赞的微博的功能的建议性决定对其单独进行检索，主要思路为kv。

思路 
1.	历史数据，采用strdict实现单机检索，按照每条微博占12字节计算，单机（按照内存48G计算）可以存储40亿条赞，同时对每个用户保存最新的1000条微博，单机即能cover 近一年数据
2.	实时数据，采用memcache(lib64/cache/memory_cache)实现，内部采用一写多读

排序 
1.	采用此条微博最新赞的时间排序，为了加快排序方式，利用N堆保存当前top-k结果 ，直接跳过更老赞的微博，O（N）时间复杂度。

性能 
1.	构造3000个关注人，每个关注人赞1000条的极端情况，总体时间约20ms（检索约10ms，排序约10ms），大部分情况下检索约几ms

进展 
1.	完成历史微博索引设计、build开发及search，目前正在build历史数据
2.	实时检索完成方案调研，待开发

----------------------------------------------------------------------------------  

每天赞的微博数据1240万，涉及4053577用户，平均每人赞3次，58%用户每天赞一次，17%赞2次，7.48388%用户每天赞3次，4%用户每天赞4次，总计90%用户每天赞的数量小于6次，大于100次用户占比0.1%。十万分之二用户赞居然超过1000条（估计是机器用户）。
根据这个数据分布，可以先给用户分配5个赞的空间，这样90%的用户可能满足需求，超过5次，则采用多个块来满足存储需求，对于实时数据每个用户最多存储最新100条即可。
对于新增uid，插入一个新node（同时能满足5次赞存储）；
对于已有uid，新增一个赞，如果没有满5次，则插入到固定内存，为了避免加锁，实现细节如下：检索时取出所有赞，同时对时间进行判断，需要满足近期一天，插入时不加锁，对于新增的赞（如果已有的赞都满足时间要求，则尾部插入，先写mid，再写time，最后修改次数，由于time，次数都是int类型，近乎一次原子操作，所以几乎不会发生数据不一致问题，如果已有的赞有部分过期，则替换原有的，先将过期的时间写0，然后写新的mid，再写时间）
对于已有mid，满5次，则分配一个块，读写操作同上
这样通过不加锁来实现并行化，对于淘汰，采用FIFO，先保证每天能存储1000万uid的赞，能存储几天内的实时数据，确保历史库overlap

----------------------------------------------------------------------------------  

另一个候选方案是利用内存搜索，把某个人赞的微博当成 某个人发的微博一样，只是无需内容字段，利用赞的时间当成微博时间，检索时 利用ids语法即可，不过内存搜索没有返回mid，内部也比较复杂，一些不必要的流程和逻辑可能会影响检索。
考虑到 实时部分还未开始（增加实时建索引以及与历史数据的合理衔接，这块工作量估计在1.5天），今天能达到的状态是：搜个人微博利用接口，能保证实时性；搜好友/关注人 赞 的微博 可能做不到完全实时性，如果build索引比较顺利，大概能做到小时级，如果 赞的微博今天上线，可能还是利用调用接口方式上，换成自己服务可能需要到周二下午。
  
----------------------------------------------------------------------------------  

实时数据与历史数据overlap方式如下：
1.	每天凌晨4点开始合并所有历史数据，此时，停止实时数据build，新赞数据堆积在队列中，每天凌晨4点赞的行为总计7万，这个时刻检索量较低，新赞也很低，停止一小时更新可以接受
2.	等到新索引build后，重启服务，开始新数据更新（这个时候堆积的新数据都在队列中，没有丢失）
后续改进方案是： 
1.	重建历史数据时不停掉实时数据，此时插入实时数据时 写入日志
2.	load历史数据时，先把日志文件插入到实时索引中（这个是重建索引阶段漏的新数据），然后读队列中的实时数据，插入到实时索引即可

----------------------------------------------------------------------------------  
  
最终实现的实时检索方案如下：

索引结构 
1.	由于每一条记录都是结构化数据，而最终查询都是统一到 uid 上，因此在索引结构设计时充分利用了倒排+正排的优势，对uid进行索引，加快 某某赞过的微博的查询速度，同时对每一条记录按照动作时间进行FIFO（循环双向队列），方便数据淘汰（目前保留25小时）
2.	value，由于每个uid赞过的微博分布大部分集中在5条以内，因此采用分块存储，每一块存储5条记录，这样保证90%以上的用户在检索时读取一个分块就可以了，每个用户最多保存最新的1000条微博，这样就较好的折中了性能与空间开销，为了方便数据块的管理，同样采用双向链表进行管理
3.	内存分配机制，虽然采用数据块，但是并不每次都malloc，而是直接分配一块大内存，在这个大内存基础上通过 data_node 来管理，data node 之间是双向链表进行管理
4.	为了方便的dump 索引文件，所有双向链表实现都采用的是 node 序号而非指针，这样在索引dump时 直接写入三块大内存即可（一hash、二datanode、三data）

索引操作 
1.	为了方便在da框架使用（提高可重用性），realtimeindex 集成BaseResource 基类，这样在插件使用时 直接通过 Resource::getResource<RealtimeIndex> ("rtindex") 就行了
2.	为了提高索引的并发读写，采用读写锁，多读一写方式，对于写操作，大致流程如下： 
	删除一个赞；查询 索引文件，如果用户赞了某个objectid，而刚好又把它删除了，直接 把time 修改为0（time 为uint32_t)，几乎一次原子操作，这样 就无需加锁了，检索时如果time为0则直接忽略
	删除一个uid；直接操作索引即可
	增加一个(uid, mid, time)，加读锁，读取索引，如果不在索引中，直接添加，此时 这条记录会插入在最新的拉链中，同时在索引node中记录下最新时间（方便淘汰）；如果在索引中，先遍历索引数据，忽略失效的数据，合并成一个新索引，然后删除 旧记录，插入一条新记录，记录时间
	进行增加操作时，先加读锁，不影响查询；然后本地进行索引合并，最后才加写锁，同时更新记录顺序（把最新的记录放到对尾，这样保证淘汰的数据永远是最久的，正因为此，在操作删除时，没有先合并索引，再删再加，而是原内存上进行标记（把time改为0），保证了记录位置不变，维持数据淘汰的正确

索引备份与dump 
1.	防止程序异常退出，设置dump周期，每隔一定周期dump出一份索引文件，当程序挂掉时，加载最新索引
2.	后续可以增加一个日志功能，把所有操作记录下来，服务恢复时先根据日志 进行数据恢复，恢复数据阶段da框架保证了服务不会开启服务端口，保证了残缺服务不提供线上正式查询
3.	可以给服务发送退出命令，服务退出前先dump 索引，然后退出，这样索引文件没有丢失，没有读取的实时数据堆积在队列中

实时索引与历史数据的overlap 
1.	目前很简单，更新历史数据时，先让程序dump实时索引，然后退出，此时，最新数据堆积在队列中，更新历史索引后，重启服务，先加载历史索引，然后加载实时索引，提供服务，之后读取队列数据进行索引更新
2.	目前更新索引服务重启时间大概在30秒左右，部署两台服务，错开启动能规避全部服务不可用的问题
  
----------------------------------------------------------------------------------  

目前使用一年数据（索引总计18G)，查询日志中查询时间超过10毫秒的大约占比6%，全部在100ms以内，为了更好解决实时、历史数据问题后续可以改进点如下：
一、实时与历史服务分离，不过需要增加一个类似bc的合并器
二、实时索引采用共享内存方式即可，目前实时索引一共用到3块连续内存，改为共享内存方式非常容易，而且服务挂掉直接重启即可（只要索引没有问题，机器内存没有问题），当然此时也可以保留 周期性索引dump以及日志dump，以用于数据恢复，后续安排看这个服务的应用范围。
  
----------------------------------------------------------------------------------  

1.	完成周期性索引dump
2.	完成从dump文件恢复功能
3.	新增加通过备份日志 rollback 功能，目前测试的性能是：finish rollback 5430405 record cost 10 s ，插入速度将近55万每秒，一天微博赞的数据约1200万条，即便服务宕机，恢复时间也只有24秒+历史数据load时间.

历史数据与实时数据衔接方案如下：
1. 每天凌晨3点，启动历史库合库程序，记录 和库时间点
2. 完成历史库merger后，推送到线上，修改 rollback时间与rollback文件
3. 重启服务ok
 
